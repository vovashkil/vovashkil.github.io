{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Call Bedrock models with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, you will create tools to sell more tickets and book shows. In the process of asking Bedrock LLMs questions and assigning them tasks to run booking, you should compare the code needed to invoke models through an AWS SDK (Boto3) and LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Choose **Select Kernel** in the top right corner and then in the **Select kernel** dialog that appears select **Install/Enable suggested extensions Python + Jupyter**.\n",
    "\n",
    "    **Note:** If you see a popup window, choose **Trust Publisher & Install**.\n",
    "\n",
    "2. Choose **Select Kernel** again, then in the **Select kernel** dialog choose **Python environments**. Then choose **Create Python Environment** and choose **Venv**, then choose **Python 3.12.11 64-bit**.\n",
    "\n",
    "3. To install required the Python libraries for this lab, select **requirements.txt**, then press **OK**.\n",
    "\n",
    "    **Note:** LangChain has multiple different packages for different purposes.\n",
    "    In this lab, you use:\n",
    "    - langchain: For its output parsers\n",
    "    - langchain-community: For its document loaders\n",
    "    - langchain_aws: For its AWS models\n",
    "    - langchain_core: For its prompt templates and output parsers\n",
    "    \n",
    "    A few notifications appear, indicating that pip is being upgraded and other packages are being installed.\n",
    "\n",
    "    Within a short time, where the notebook previously displayed the \"Select Kernel\" message, it should now display \".venv (Python 3.12.11)\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. To suppress unnecessary warnings, run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** When you place your cursor in a code cell, a **play** icon will appear on the left side. Use that to run the code. You will know the code block has completed running when you see a number display within square brackets below the play icon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1: Invoke models using boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use *AWS SDK for Python (Boto3)* to ask an Amazon Nova Lite model how to get more people in the door."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. To help the venue think of ideas to sell more tickets, perform a text completion task with *Amazon Nova Lite* through Boto3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "from IPython.display import Markdown\n",
    "\n",
    "\n",
    "# Create the Amazon Bedrock client\n",
    "bedrock_client = boto3.client('bedrock-runtime')\n",
    "\n",
    "# Define the text generation configuration\n",
    "textGenerationConfig = {\n",
    "    \"maxTokens\": 512,\n",
    "    \"temperature\": 0.5,\n",
    "    \"topP\": 0.9\n",
    "}\n",
    "\n",
    "modelId = \"amazon.nova-lite-v1:0\"\n",
    "\n",
    "# Define the input text for text generation\n",
    "prompt = \"A music venue can sell out every night by...\"\n",
    "\n",
    "native_request = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": prompt}]\n",
    "        }\n",
    "    ],\n",
    "    \"inferenceConfig\": textGenerationConfig\n",
    "}\n",
    "\n",
    "request = json.dumps(native_request).encode('utf-8')\n",
    "\n",
    "# Invoke the Amazon Bedrock model for text generation\n",
    "response = bedrock_client.invoke_model(modelId=modelId, body=request)\n",
    "\n",
    "# Extract the outputText from the response\n",
    "response = json.loads(response[\"body\"].read())\n",
    "generated_text = response.get('output').get('message').get('content')[0].get('text')\n",
    "\n",
    "# Print the generated text\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model invocation with Boto3 can be seen as low-level. If you want to make a complex, production-ready LLM tool, it requires you to do lots of things yourself in Python.\n",
    "\n",
    "See the length of code required to invoke above and the return object that requires indexing to print pretty as some examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2: Invoke models using ChatBedrock with simple prompts\n",
    "\n",
    "Leveraging LangChain components and abstractions to invoke Amazon Bedrock models can make your code more high-level. LangChain handles the low-level API details, response parsing, and error handling, allowing you to focus on building your application logic rather than managing model integration complexities.\n",
    "\n",
    "You will use Amazon Nova Lite through LangChain's ChatBedrock component, which handles both single prompts and multi-turn conversations effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Create a LangChain ChatBedrock component that uses *Amazon Nova Lite*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the ChatBedrock class from the langchain_aws.chat_models.bedrock module\n",
    "from langchain_aws.chat_models.bedrock import ChatBedrock\n",
    "\n",
    "# Set the modelId to the desired model ID (in this case, \"amazon.nova-lite-v1:0\")\n",
    "modelId = \"amazon.nova-lite-v1:0\"\n",
    "\n",
    "# Create an instance of the ChatBedrock with the specified model ID\n",
    "nova_llm = ChatBedrock(model_id=modelId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. To brainstorm bands to reach out to, ask the LLM what the top selling bands of all time are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt to be sent to the LLM\n",
    "prompt = \"What are the top-selling bands of all time?\"\n",
    "\n",
    "# Call the invoke method of the ChatBedrock instance with the prompt formatted as a human message and store the response\n",
    "response = nova_llm.invoke([(\"human\", prompt)])\n",
    "\n",
    "# Render the response content as Markdown using the Markdown function\n",
    "Markdown(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe this independent venue should be realistic about their booking potential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Prompt the LLM again, this time, with a caveat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What are 10 bands right now that only play small venues? Describe each band in detail.\"\n",
    "\n",
    "# Call the invoke method of the ChatBedrock instance with the prompt formatted as a human message and store the response\n",
    "response = nova_llm.invoke([(\"human\", prompt)])\n",
    "\n",
    "# Render the response content as Markdown using the Markdown function\n",
    "Markdown(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.3: Invoke models using ChatBedrock with structured messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use LangChain to work with Bedrock models in a more conversational way with structured messages. LangChain chat models use a sequence of messages as inputs and return messages (as opposed to plain text) as outputs. You will use LangChain's chat model component, ChatBedrock, to book shows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with a chat model, each message contains a *role* and *content*. In this lab, you work with the following:\n",
    "- Human messages: Represents a message from a user\n",
    "- AI messages: Represents a message from a model\n",
    "- System messages: Represents a system messages, which tells the model how to behave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Import the ChatBedrock class, create an inference request parameters object, and define the Bedrock model id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the ChatBedrock class from the langchain_aws.chat_models.bedrock module\n",
    "from langchain_aws.chat_models.bedrock import ChatBedrock\n",
    "\n",
    "# Define the text generation configuration\n",
    "textGenerationConfig = {\n",
    "    \"maxTokens\": 512,\n",
    "    \"temperature\": 0.5,\n",
    "    \"topP\": 0.9\n",
    "}\n",
    "\n",
    "modelId = \"amazon.nova-lite-v1:0\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. **Challenge:** Create a **ChatBedrock** object, assigning it to a variable called **chat**, using **textGenerationConfig** and **modelId** as the inputs to the necessary parameters\n",
    "[LangChain API Reference](https://python.langchain.com/v0.2/api_reference/aws/chat_models/langchain_aws.chat_models.bedrock.ChatBedrock.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Amazon Nova Lite LangChain chat model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>Solution:</b> Select if you need to see the solution.</summary>\n",
    "    \n",
    "<br/>\n",
    "\n",
    "```python\n",
    "# Create an Amazon Nova chat model object\n",
    "nova_chat = ChatBedrock(\n",
    "    client=bedrock_client,\n",
    "    model_id=modelId,\n",
    "    model_kwargs=textGenerationConfig,\n",
    ")\n",
    "\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain chat models work with lists of messages, each with a persona and content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Because you are working with a chat model, create a messages list with a *system message*, describing the model's purpose to fulfill, and a *content*, the question or response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a messages list to join in on the middle of a conversation with the chat model\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are the manager of a music venue. You respond to artists who reach out to you about playing a show at your venue on their upcoming tour.\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\", \n",
    "        \"Hello! We are an up-and-coming punk band with thousands of fans. We are coming to town September 17\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Invoke the chat model, using the messages list as a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unlike, an LLM object, LangChain chat models, take a list of messages as a parameter for invocation.\n",
    "ai_msg = nova_chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learn more:** See [LangChain ChatBedrock documentation](https://python.langchain.com/v0.2/api_reference/aws/chat_models/langchain_aws.chat_models.bedrock.ChatBedrock.html) for a list of methods and ways to invoke this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AI response comes in *Message* format. To avoid printing metadata and persona, index to the *content* key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. Print the content of the AI response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(ai_msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.4: Challenge: Calculate cost of invocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use LangChain methods and Bedrock documentation to calculate the cost of your last LLM invocation.\n",
    "\n",
    "When preparing to launch LLM applications, it is important to have an understanding of cost. If used by large organizations or repeatedly, Amazon Bedrock usage costs can add up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  Find a method in the [LangChain documentation](https://api.python.langchain.com/en/latest/chat_models/langchain_aws.chat_models.bedrock.ChatBedrock.html) that returns the number of tokens present in a text string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  Use that method on **prompt** and assign the output to a variable called **input_tokens**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the number of tokens used in prompt to a variable called input_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18. Use the same method on **ai_message.content** and assign the output to a variable called **output_tokens**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the number of tokens used in ai_message.content to a variable called output_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>Solution:</b> Select if you need to see the solution. If a warning message appears below the cell after you run it, you can safely ignore the message or run the cell again. </summary>\n",
    "    \n",
    "<br/>\n",
    "\n",
    "```python\n",
    "# The number of tokens the prompt uses for Amazon Nova Lite\n",
    "input_tokens = nova_chat.get_num_tokens(prompt)\n",
    "# The number of tokens the AI response used for Amazon Nova Lite\n",
    "output_tokens = nova_chat.get_num_tokens(ai_msg.content)\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19. Scan through [Amazon Bedrock pricing](https://aws.amazon.com/bedrock/pricing/) to find (and assign) the following values for **Amazon Nova Lite** (On-Demand, US East):\n",
    "\n",
    "- **Price per 1,000 input tokens**, assigning that rate to variable **input_price** (price/1000)\n",
    "- **Price per 1,000 output tokens**, assigning that rate to variable **output_price** (price/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two new variables for respective token prices\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>Solution:</b>Select if you need to see the solution.</summary>\n",
    "    \n",
    "<br/>\n",
    "\n",
    "```python\n",
    "# The listed price per 1,000 input tokens for using Amazon Nova Lite, divided by 1,000, to get the price of a single token\n",
    "input_price = .0002/1000\n",
    "# The price per 1,000 output tokens for using Amazon Nova Lite, divided by 1,000, to get the price of a single token\n",
    "output_price = .0006/1000\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20. Create an equation to calculate the cost of the model invocation in dollars and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the cost of the model invocation (input and output)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>Solution:</b> Select if you need to see the solution.</summary>\n",
    "    \n",
    "<br/>\n",
    "\n",
    "```python\n",
    "# Cost = quantity * price\n",
    "cost_cents = input_tokens * input_price + output_tokens * output_price\n",
    "# Multiply by 100 to go from cents to dollars\n",
    "cost_dollars = cost_cents * 100\n",
    "(f\"The price is ${cost_dollars}\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While, the cost of this one invocation is very small, it's important to have an understanding of such costs. Such an invocation, compounded thousands or millions of times for a large organization or programmatic task, could have a massive cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task complete**: You built tools for the venue to sell more tickets and book shows using LangChain models and their methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 4: Explore LangChain class capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, you will create parts of AI tools to help make the venue bar manager's job easier.\n",
    "You'll use some new basic tools of LangChain, Messages, Prompt Templates, and Parsers, that abstract low level code to do common tasks when working to productionalize AI applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.1: Messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last task you worked with messages as a list of tuples. To simplify working with chat models, LangChain Core offers messages classes. Use messages to communicate with a chat model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, messages passed to a chat model should start with a *SystemMessage*. A SystemMessage can the general task and guidelines a chat model should adhere to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21. Create a list of message objects:\n",
    "- Priming the model with **SystemMessage**.\n",
    "- Conveying the first message from a user with **HumanMessage**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are an AI assistant helping a music venue bar with administrative tasks.\"),\n",
    "    HumanMessage(content=\"Draft an email to all bar staff reminding them of the updated closing procedures and checklists that need to be completed each night after events.\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22. Invoke the chat model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = nova_chat.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also stream chunks of the model response for a more pleasant, faster user experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "23. Invoke the model again, streaming its response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the stream of responses from the AI model\n",
    "for chunk in nova_chat.stream(prompt):\n",
    "    # Extract and print the text content from Nova's structured response\n",
    "    if hasattr(chunk, 'content') and chunk.content:\n",
    "        if isinstance(chunk.content, list):\n",
    "            for item in chunk.content:\n",
    "                if isinstance(item, dict) and item.get('type') == 'text':\n",
    "                    # Print the content of each response chunk without adding a newline character\n",
    "                    # end=\"\" prevents printing a newline after each chunk\n",
    "                    # flush=True ensures the output is flushed immediately, without buffering\n",
    "                    print(item.get('text', ''), end=\"\", flush=True)\n",
    "        else:\n",
    "            print(chunk.content, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.2: Prompt templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain Prompt templates allow you to create reusable prompts with placeholders that can be filled with specific inputs, separating the structure of the prompt from the data. Use prompt templates to create purchase orders and come up with new drinks to put on the menu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **PromptTemplate** can be used with llm objects to inject inputs into a standard prompt design, creating prompts as strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "24. To use a prompt template to create several prompts dynamically, run the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\"What is the capital of {state}?\")\n",
    "\n",
    "prompt = prompt_template.format(state = \"Nebraska\")\n",
    "print(\"Prompt 1: \", prompt)\n",
    "\n",
    "prompt = prompt_template.format(state = \"New York\")\n",
    "print(\"Prompt 2: \", prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets create a tool for the bar manager using prompt templates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25. Create a list of orders the bar needs to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = [\n",
    "    {\n",
    "        \"product\" : \"soda\",\n",
    "        \"supplier\" : \"TheSodaCompany, LLC\",\n",
    "        \"date\" : \"9/10/2024\"\n",
    "    },\n",
    "    {\n",
    "        \"product\" : \"napkins\",\n",
    "        \"supplier\" : \"Napkin Inc.\",\n",
    "        \"date\" : \"9/12/2024\"\n",
    "    },\n",
    "    {\n",
    "        \"product\" : \"receipt paper\",\n",
    "        \"supplier\" : \"Paper Unlimited\",\n",
    "        \"date\" : \"9/19/2024\"\n",
    "    }\n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will use each dictionary in the orders list to generate prompts, substituting the placeholders {product}, {supplier}, and {date} with the corresponding values from each dictionary.\n",
    "\n",
    "In the following template, there are placeholders (i.e. the product and supplier and date) that match with keys in the dictionaries inside the order list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "26. Create a reusable prompt template that makes purchase orders to the venues supplier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template=\"\"\"\n",
    "Human: Create a purchase order for {product} to {supplier} from our company, AMusicVenue,\n",
    "stating that we require delivery by {date}.\n",
    "\n",
    "Assistant:\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables, in {}, allow prompts to be reused by replacing the placeholders with real values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "27. Turn the string into a PromptTemplate object, using the *from_template()* method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate.from_template(template)\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A prompt template allows you to parameterize prompts, making them reusable. To turn the prompt template into a prompt, you assign values into the variables within the template using *format()* or *invoke()*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "28. To create a prompt, use that prompt to invoke a Bedrock model, and print the response, for each of your three needed product orders, run the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nova_llm = ChatBedrock(model_id=\"amazon.nova-lite-v1:0\",\n",
    "                         model_kwargs={\n",
    "                             \"temperature\": .8,\n",
    "                             \"topP\": .8\n",
    "                         })\n",
    "\n",
    "# PO Counter\n",
    "order_num = 1\n",
    "\n",
    "# Loop through the orders list\n",
    "for order in orders:\n",
    "    # Create a prompt from a template by indexing the order dict\n",
    "    prompt = prompt_template.format(product=order[\"product\"],\n",
    "                             supplier=order[\"supplier\"],\n",
    "                             date=order[\"date\"])\n",
    "    # Invoke a Bedrock model\n",
    "    response = nova_llm.invoke([(\"human\", prompt)])\n",
    "\n",
    "    # Print the order #\n",
    "    print(\"PO #\" + str(order_num) + \"\\n\")\n",
    "    # Increment the order # counter\n",
    "    order_num += 1\n",
    "    # Print the model response\n",
    "    print(response.content)\n",
    "    print (\"---------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** This code generated purchase order prompts by substituting placeholders in the template with values from a list of orders. It sent the prompts to an Amazon Bedrock LLM and printed the model's responses along with the purchase order numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are the bars purchase orders to its vendors for the week!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **ChatPromptTemplate** is a tool that allows you to define a reusable template for conversational prompts, consisting of multiple messages with designated roles (e.g., system, user, assistant). It provides a structured way to incorporate user inputs or placeholders into the message list, which can then be used with LangChain chat models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "29. To create a chat prompt template to help the bar manager when they have to make a drink they don't know, create the following template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the ChatPromptTemplate class from the langchain_core.prompts module\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate(\n",
    "  [\n",
    "    # Define the initial system message for the chat prompt\n",
    "    (\"system\", \"You are a bar-keeper's assistant. When a user says a drink name, you respond briefly, giving them only the ingredients\"),\n",
    "    # Define the user's input message for the chat prompt\n",
    "    # The {drink} placeholder will be substituted with the actual drink name provided by the user\n",
    "    (\"user\", \"{drink}\"),\n",
    "   ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "30. Create a prompt from the prompt template, which will prompt the chat model to list the ingredients for a delicious drink."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a formatted prompt by substituting \"shirley temple\" for the {drink} placeholder in the chat_template\n",
    "prompt = chat_template.invoke(\"shirley temple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "31. Stream the response from the chat model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the stream of responses from the AI model\n",
    "for chunk in nova_chat.stream(prompt):\n",
    "    # Extract and print the text content from Nova's structured response\n",
    "    if hasattr(chunk, 'content') and chunk.content:\n",
    "        if isinstance(chunk.content, list):\n",
    "            for item in chunk.content:\n",
    "                if isinstance(item, dict) and item.get('type') == 'text':\n",
    "                    # Print the content of each response chunk without adding a newline character\n",
    "                    # end=\"\" prevents printing a newline after each chunk\n",
    "                    # flush=True ensures the output is flushed immediately, without buffering\n",
    "                    print(item.get('text', ''), end=\"\", flush=True)\n",
    "        else:\n",
    "            print(chunk.content, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.3: Output parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language models typically produce unstructured text outputs. When your application requires structured data, in a specific format, you need to prompt LLMs for very specific formats, which they might be resistant to, or transform outputs to meet your requirements. LangChain output parsers can be used to transform the output of an LLM to a more suitable format. \n",
    "\n",
    "Use output parsers to get outputs of specific formats from LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **CommaSeperatedListOutputParser** component provides several abilities. You can:\n",
    "- Add its format instructions to a prompt template, which will inform the chat model on what to return. Doing such will add the following to your prompt: \"Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`\".\n",
    "- Invoke it to parse the model response from a string to a list of strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a bartender runs out of an ingredient on a show night, they might have to find another way to make a Shirley Temple on the fly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "32. Create a prompt template to ask the chat model for a commas separated list of potential replacements for an ingredient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the CommaSeparatedListOutputParser class from the langchain.output_parsers module\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "# Create an instance of the CommaSeparatedListOutputParser\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "# Get the format instructions from the output parser\n",
    "# This will return a string with instructions for the language model\n",
    "# to format its output as a comma-separated list\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "# Create a PromptTemplate object\n",
    "# The template string includes a placeholder for the format instructions\n",
    "# and a placeholder for the input variable \"ingredient\"\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"List substitutes for {ingredient}.\\n{format_instructions}\",\n",
    "    input_variables=[\"ingredient\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "33. Create a prompt from the prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the PromptTemplate object to create a prompt for the language model\n",
    "# by invoking it with the input variable \"ingredient\" set to \"grenadine\"\n",
    "prompt = prompt_template.invoke({\"ingredient\": \"grenadine\"})\n",
    "\n",
    "# Print the generated prompt string\n",
    "print(prompt.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "34. Observe the model response before CommaSeparatedListOutputParser is invoked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nova_chat = ChatBedrock(model_id=\"amazon.nova-lite-v1:0\", model_kwargs = {\n",
    "    \"temperature\": .2,\n",
    "    \"top_p\": .2})\n",
    "\n",
    "response = nova_chat.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "35. Invoke the CommaSeparatedListOutputParser to create a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_parser.invoke(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task complete**: You made tools for the bar manager using LangChain components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 5: Create a chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, you will leverage the LangChain components you've already learned, to build a chatbot to help organize shifts for venue staff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain chat models are ideal for building chatbots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "36. To relay its directive to the chatbot, define a system message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = SystemMessage(content=\"You are a chatbot built for scheduling staff shifts for AMusicVenue, an independent music venue.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "37. **Challenge**: Finish the shift booking chatbot that will invoke Mistral Large with a system message and a user input\n",
    "\n",
    "    - Replace the `<FMI-1>` \"Fill Me In (FMI)\" value in the code with the appropriate message and its required parameter.\n",
    "\n",
    "    **Tip:** Look for the message type to take a human message in [Langchain_core documentation](https://api.python.langchain.com/en/latest/core_api_reference.html#module-langchain_core.messages) and then find what parameter it takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nova_chat = ChatBedrock(model_id=\"amazon.nova-lite-v1:0\")\n",
    "\n",
    "try:\n",
    "    # Print a welcome message to the user\n",
    "    print(\"Welcome to this simple chatbot! To exit, choose the interrupt button and then press esc.\")\n",
    "    while True:\n",
    "        # Prompt the user for input\n",
    "        user_input = input(\"User: \")\n",
    "        # Create a HumanMessage object with the user's input\n",
    "        human_message = <FMI-1>\n",
    "        # Print the user's input\n",
    "        print(f\"\\nHuman: {user_input}\")       \n",
    "        print(\"AI: \", end=\"\")\n",
    "        \n",
    "        # Stream the response from the chatbot\n",
    "        for chunk in nova_chat.stream([system_message, human_message]):\n",
    "            # Extract text from Nova's structured response format\n",
    "            if hasattr(chunk, 'content') and chunk.content:\n",
    "                if isinstance(chunk.content, list):\n",
    "                    for item in chunk.content:\n",
    "                        if isinstance(item, dict) and item.get('type') == 'text':\n",
    "                            print(item.get('text', ''), end=\"\", flush=True)\n",
    "                else:\n",
    "                    print(chunk.content, end=\"\", flush=True)\n",
    "        \n",
    "        print(\"\\n\")  # Add newline after response\n",
    "\n",
    "# Handle the KeyboardInterrupt exception (when the user presses esc)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>Solution:</b> Select if you need to see the solution.</summary>\n",
    "    \n",
    "<br/>\n",
    "\n",
    "```python\n",
    "human_message = HumanMessage(content = user_input)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "38. Start the chatbot by running the above cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "39. In the input pop-up at the top of the IDE, input the initial message \"This is *your name*, I want to work Saturday.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "40. Continue to respond to its questions, poking holes at its knowledge, until you see model is clearly confused.\n",
    "- If it says it will check on availability, ask what it found.\n",
    "- If the chatbot claims to schedule you for a shift, ask what position the shift is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "41. To exit, choose **Interrupt** from the main notebook editor toolbar, and then press the **Esc**/**esc** key.\n",
    "\n",
    "**Warning:** If a pop up appears asking if \"want to restart the kernel\", choose **cancel** and then press the the **Esc**/**esc** key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/interrupt.png\">\n",
    "\n",
    "***Image description**: The *Interrupt* key within the IDE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You likely had an interaction with the chatbot that hardly resembled a human interaction and was inadequate as a tool for the venue to use.\n",
    "\n",
    "The chatbot has two primary flaws:\n",
    "- It lacks statefullness, meaning it has no recollection of previous messages that you or it have delivered. It will have forgotten your name and what day you want to work by the time the next response is sent.\n",
    "- It hallucinates, as it has no awarness of shift availability, so it tends to make information up.\n",
    "\n",
    "In the next task, you will fix those two flaws."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task complete**: You created a chatbot for staffing the venue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Task 6: Make the chatbot application specific"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, you will add statefullness and context of needed shifts to the chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6.1: Make the chatbot stateful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statelessness means LLMs do not retrain messages for their next invocation. Without the previous messages to the chatbot being stored, an LLMs utility is limited to a human message and an AI response.\n",
    "So that venue staff can have a conversation with the chatbot, add statefullness to the chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "42. **Challenge**: Finish adding statefullness to shift booking chatbot by \n",
    "\n",
    "    - Replace the `<FMI-1>` \"Fill Me In (FMI)\" value in the code with the appropriate method to add an object to the *messages* list.\n",
    "    - Replace the `<FMI-2>` \"Fill Me In (FMI)\" value in the code with the appropriate parameter to add the *model response* to the messages list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a chatbot built for scheduling staff shifts for AMusicVenue, an independent music venue.\")\n",
    "]\n",
    "\n",
    "try:\n",
    "    # Print a welcome message to the user\n",
    "    print(\"Welcome to this simple chatbot! To exit, choose the interrupt button and then press esc.\")\n",
    "    while True:\n",
    "        # Prompt the user for input\n",
    "        user_input = input(\"User: \")\n",
    "        human_message = user_input\n",
    "        print(f\"Human:\\n{user_input}\\n---------------\")\n",
    "        # Create a HumanMessage object with the user's input and add it to the messages list\n",
    "        <FMI-1>(HumanMessage(human_message))\n",
    "\n",
    "        # Assign a blank string that will be added to from the response chunks\n",
    "        response_content = \"\"\n",
    "\n",
    "        print(\"AI: \")\n",
    "        # Stream the response from the chatbot and append each chunk to a string\n",
    "        for chunk in nova_chat.stream(messages):\n",
    "            # Extract text from Nova's structured response format\n",
    "            if hasattr(chunk, 'content') and chunk.content:\n",
    "                if isinstance(chunk.content, list):\n",
    "                    for item in chunk.content:\n",
    "                        if isinstance(item, dict) and item.get('type') == 'text':\n",
    "                            text = item.get('text', '')\n",
    "                            print(text, end=\"\", flush=True)\n",
    "                            response_content += text\n",
    "                else:\n",
    "                    print(chunk.content, end=\"\", flush=True)\n",
    "                    response_content += chunk.content\n",
    "        \n",
    "        # Add the AI message to the messages list\n",
    "        messages.append(<FMI-2>)\n",
    "        print(f\"\\n---------------\")\n",
    "\n",
    "# Handle the KeyboardInterrupt exception (when the user presses esc)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>Solution:</b> Select if you need to see the solution.</summary>\n",
    "    \n",
    "<br/>\n",
    "\n",
    "\n",
    "```python\n",
    "1. messages.append(HumanMessage(human_message))\n",
    "```\n",
    "\n",
    "```python\n",
    "2. messages.append(AIMessage(response_content))\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "43. Start the chatbot by running the above cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "44. In the input pop-up at the top of the IDE, input the initial message \"This is *your name*, I want to work Saturday.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "45. Continue to respond to its questions, until you see that the chatbot has gained statefullness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "46. To exit, choose **Interrupt** from the main notebook editor toolbar, and then press the **Esc**/**esc** key.\n",
    "\n",
    "**Warning:** If a pop up appears asking if \"want to restart the kernel\", choose **cancel** and then press the the **Esc**/**esc** key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/interrupt.png\">\n",
    "\n",
    "***Image description**: The *Interrupt* key within the IDE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! The chatbot has gained more function and is now stateful. But, it still lacks awareness of open shifts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6.2: Give the chatbot context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On their own, LLMs are a fantastic tool. They are trained on mass amounts of data from the internet, making them incredibly knowledgeable. But, they don't know data specific to your use case. Add the functionality to pull data from a shift tracking CSV file to be able to effectively schedule shifts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Document loaders* are another way to add functionality to LangChain models. They load data from various sources, like AWS S3, JSON files, and URLs, into document objects. You can use those documents to provide context for model invocation.\n",
    "\n",
    "There is a file in your directory called *shifts.csv*. It is a comma-seperated values file containing all remaining shifts the venue needs to staff for a given week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "47. To examine that file, select **shifts.csv** from the IDE sidebar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "48. To load the CSV file, with each line/available shift as its own document, run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "loader = CSVLoader(file_path=\"./shifts.csv\")\n",
    "\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "49. To make the documents usable for your purpose, iterate through them, splitting each *page_content* string and storing them in a list of shift dictionary objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the shift dictionaries\n",
    "shifts = []\n",
    "\n",
    "# Iterate over each document in the loaded documents\n",
    "for document in documents:\n",
    "    # Get the page content of the current document as a string\n",
    "    data_str = document.page_content\n",
    "    # Create a dictionary from the string, splitting on newline ('\\n') and colon (': ')\n",
    "    # The keys of the dictionary are the parts before the colon, converted to lowercase\n",
    "    # The values of the dictionary are the parts after the colon\n",
    "    data_dict = {line.split(': ')[0].lower(): line.split(': ')[1] for line in data_str.split('\\n')}\n",
    "    # Add the newly created dictionary to the shifts list\n",
    "    shifts.append(data_dict)\n",
    "\n",
    "shifts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "50. To turn the list of dicts into a string usable by the chat model, run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty string to store the shift information\n",
    "shifts_string = \"\"\n",
    "# Iterate over each shift dictionary in the shifts list\n",
    "for shift in shifts:\n",
    "    # If taken key does not equal taken, the shift is available\n",
    "    if not(shift['taken'].lower() == 'taken'):\n",
    "        # Ex: Front door needs staffing on Saturday, 5-CLOSE\n",
    "        textrep = \"{} needs staffing on {}, {}.\".format(shift['duty'],shift['day'],shift['time'])\n",
    "    # Shift is taken\n",
    "    else:\n",
    "        #Ex: Bar does not need staffing on Friday, 6-10, because Alejandro Rosalez is working it.\n",
    "        textrep = \"{} does not need staffing on {}, {}, because {} is working it.\".format(shift['duty'],shift['day'],shift['time'],shift['name'])\n",
    "    # Add the shift to the string of all shifts\n",
    "    shifts_string += textrep + \"\\n\"\n",
    "print(shifts_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "51. **Challenge:** To give the chatbot the necessary context, add the string containing shifts, **shifts_string**, to the *SystemMessage*, and then start the chatbot by running the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are a chatbot built for scheduling staff shifts for AMusicVenue, an independent music venue. You tell staff what shifts are available when they ask or that that they are not. These are the shifts this week:\")\n",
    "]\n",
    "\n",
    "try:\n",
    "    # Print a welcome message to the user\n",
    "    print(\"Welcome to this simple chatbot! To exit, choose the interrupt button and then press esc.\")\n",
    "    while True:\n",
    "        # Prompt the user for input\n",
    "        user_input = input(\"User: \")\n",
    "        human_message = user_input\n",
    "        print(f\"Human:\\n{user_input}\\n---------------\")\n",
    "        # Create a HumanMessage object with the user's input and add it to the messages list\n",
    "        messages.append(HumanMessage(human_message))\n",
    "\n",
    "        # Assign a blank string that will be added to from the response chunks\n",
    "        response_content = \"\"\n",
    "\n",
    "        print(\"AI: \")\n",
    "        # Stream the response from the chatbot and append each chunk to a string\n",
    "        for chunk in nova_chat.stream(messages):\n",
    "            # Extract text from Nova's structured response format\n",
    "            if hasattr(chunk, 'content') and chunk.content:\n",
    "                if isinstance(chunk.content, list):\n",
    "                    for item in chunk.content:\n",
    "                        if isinstance(item, dict) and item.get('type') == 'text':\n",
    "                            text = item.get('text', '')\n",
    "                            print(text, end=\"\", flush=True)\n",
    "                            response_content += text\n",
    "                else:\n",
    "                    print(chunk.content, end=\"\", flush=True)\n",
    "                    response_content += chunk.content\n",
    "        ai_message = AIMessage(response_content)\n",
    "\n",
    "        # Append the AI message to the messages list\n",
    "        messages.append(ai_message)\n",
    "        print(f\"\\n---------------\")\n",
    "\n",
    "# Handle the KeyboardInterrupt exception (when the user presses esc)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>Solution:</b> Select if you need to see the solution.</summary>\n",
    "    \n",
    "<br/>\n",
    "\n",
    "```python\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a chatbot built for scheduling staff shifts for AMusicVenue, an independent music venue. You tell staff what shifts are available when they ask or that that they are not. These are the shifts this week:\" + shifts_string)\n",
    "]\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "52. Input the initial message \"This is *your name*, I want to work Saturday.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "53. Continue to respond to its questions, seeing that the chatbot now has the context required to make intelligent staffing decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "54. To exit, choose **Interrupt** from the main notebook editor toolbar, and then press the **Esc**/**esc** key.\n",
    "\n",
    "**Warning:** If a pop up appears asking if \"want to restart the kernel\", choose **cancel** and then press the the **Esc**/**esc** key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/interrupt.png\">\n",
    "\n",
    "***Image description**: The *Interrupt* key within the IDE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain has the tools to go even further: handling the logic, parsing, and secondary model invocation that would be required to update *shifts.csv* after a staff-member has agreed to take a shift, but you will not be learning those in this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task complete**: You used LangChain to add statefullness and context to your staffing chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "You have completed this notebook. To continue to the next part of the lab, do the following:\n",
    "\n",
    "- Close this notebook file.\n",
    "- Return to the lab instructions and continue with the **Conclusion** section."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
